import pandas as pd
import os
import json
from nltk.corpus import wordnet as wn
from typing import Dict, List

def get_DB_path():
    """
    Constructs and returns the path to the 'dbs' directory, which is located in the parent
    directory of the script currently being executed.

    Returns:
        str: The absolute path to the 'dbs' directory.
    """
    DB_PATH = "\\".join(os.path.dirname(os.path.abspath(__file__)).split("\\")[:-1])
    return  os.path.join(DB_PATH, "dbs")

def get_data_from_SAaDd():
    """
    Loads data from the 'Students Anxiety and Depression Dataset.xlsx'. 
    The function attempts to open and read the dataset, converting its contents
    into a list of lists.

    Returns:
        list: A list of lists containing the data from the Excel file if file is found.
        int: Returns -1 if the file is not found.

    Raises:
        FileNotFoundError: If the Excel file is not found at the specified path.
    """
    try:
        path = os.path.join(get_DB_path(), "Students Anxiety and Depression Dataset.xlsx")
        data = pd.read_excel(path)

        return data.values.tolist() # [['oh my gosh', 1.0],...]
    
    except FileNotFoundError:
        return -1
    


def get_data_from_ddrc():
    """
    Loads data from the 'Depression_dataset_reddit_cleaned.xlsx'. 
    The function attempts to open and read the dataset, converting its contents
    into a list of lists.

    Returns:
        list: A list of lists containing the data from the Excel file if file is found.
        int: Returns -1 if the file is not found.

    Raises:
        FileNotFoundError: If the Excel file is not found at the specified path.
    """
    try:
        path = os.path.join(get_DB_path(), "depression_dataset_reddit_cleaned.csv")
        dataframe = pd.read_csv(path)
        return dataframe.values.tolist() # [['oh my gosh', 1],...]
    except FileNotFoundError:
        return -1

def create_dataset(data, file_name = "composite_db.csv"):
    """
    Creates a CSV file from the provided data and saves it to the specified file within the 'dbs' directory.
    The CSV file will include an index column automatically generated by pandas, alongside two specified
    columns: 'Data' for text data and 'Value' where 1 indicates depression and 0 indicates no depression.

    Args:
        data (list of lists): Data to be saved into the CSV file. Each sublist should contain two elements:
                            the text data and its associated binary value (1 or 0).
        file_name (str, optional): Name of the file to save the data to. Defaults to "composite_db.csv".

    Returns:
        None: The function does not return any value but writes directly to a file.
    """
    path = os.path.join(get_DB_path(), file_name)

    df = pd.DataFrame(data, columns=["Data", "Value (1 is depressed / 0 is not depressed)"])
    df.to_csv(path, index=True)

def get_data_from_composite_dataset(file_name="composite_db.csv", cases=True):
    """
    Loads data from a CSV file located in the 'dbs' directory and optionally filters the rows based
    on the depression indicator (third column in the dataset). It also ensures that the second column
    entries, expected to be strings, are correctly typed.

    Args:
        file_name (str, optional): Name of the CSV file to load data from. Defaults to "composite_db.csv".
        cases (bool, optional): Filter condition to apply on the 'Value' column which represents depression.
                                If True, only rows where the 'Value' is 1 are returned.
                                If False, only rows where the 'Value' is 0 are returned.
                                Defaults to True.

    Returns:
        list: A list of rows from the CSV file that meet the specified condition and where the second
            column's entries are strings. Each row is a list itself.
        int: Returns -1 if the CSV file is not found at the specified path.

    Raises:
        FileNotFoundError: If the CSV file is not found at the specified path.
    """
    
    try:
        path = os.path.join(get_DB_path(), file_name)
        dataframe = pd.read_csv(path)

        if cases in [True, False]:  
            dataframe = dataframe[dataframe.iloc[:, 2] == int(cases)]
        return [element for index, element in enumerate(dataframe.values.tolist()) if isinstance(element[1], str)]
    except FileNotFoundError:
        return -1

def get_utils_path():
    """
    Constructs and returns the path to the 'utils.json' file, which is located in the 'dbs' directory.
    
    Returns:
        str: The absolute path to the 'utils.json' file.
    """
    return os.path.join(get_DB_path(), "utils.json")

def write_frequent_words(freq_words):
    """
    Writes the frequent words and their counts to the 'utils.json' file. If the file doesn't exist, it creates it.
    If the file is empty or the content has changed, it updates the file with the new data.
    
    Args:
        freq_words (dict): A dictionary of words and their counts.
        
    Returns:
        None
    """
    path = get_utils_path()
    try:
        if not os.path.exists(path):
            with open(path, 'w') as file:
                json.dump({"frequent_words": freq_words}, file)
        else:
            with open(path, 'r') as file:
                data = json.load(file)
            
            if "frequent_words" not in data or data["frequent_words"] != freq_words:
                data["frequent_words"] = freq_words
                with open(path, 'w') as file:
                    json.dump(data, file)
    except FileNotFoundError:
        with open(path, 'w') as file:
            json.dump({"frequent_words": freq_words}, file)

def read_frequent_words():
    """
    Reads the frequent words and their counts from the 'utils.json' file.
    
    Returns:
        dict: A dictionary of words and their counts if found, otherwise an empty dictionary.
    """
    path = get_utils_path()
    try:
        with open(path, 'r') as file:
            data = json.load(file)
            return data.get("frequent_words", {})
    except (FileNotFoundError, json.JSONDecodeError):
        return {}

def write_best_synsets(best_depression_synsets):
    """
    Writes the best depression synsets to the 'utils.json' file. If the file doesn't exist, it creates it.
    If the file is empty or the content has changed, it updates the file with the new data.
    
    Args:
        best_depression_synsets (dict): A dictionary of words and their best synsets.
        
    Returns:
        None
    """
    path = get_utils_path()
    try:
        if not os.path.exists(path):
            with open(path, 'w') as file:
                json.dump({"best_depression_synsets": best_depression_synsets}, file)
        else:
            with open(path, 'r') as file:
                data = json.load(file)
            
            if "best_depression_synsets" not in data or data["best_depression_synsets"] != best_depression_synsets:
                data["best_depression_synsets"] = best_depression_synsets
                with open(path, 'w') as file:
                    json.dump(data, file)
    except FileNotFoundError:
        with open(path, 'w') as file:
            json.dump({"best_depression_synsets": best_depression_synsets}, file)

def read_best_synsets():
    """
    Reads the best depression synsets from the 'utils.json' file.
    
    Returns:
        dict: A dictionary of words and their best synsets if found, otherwise an empty dictionary.
    """
    path = get_utils_path()
    try:
        with open(path, 'r') as file:
            data = json.load(file)
            return data.get("best_depression_synsets", {})
    except (FileNotFoundError, json.JSONDecodeError):
        return {}
    
def write_tree_to_json(tree):
    """
    Writes the similarity tree to the 'utils.json' file under the 'sim_tree' key.
    If the file doesn't exist, it creates it.
    If the file is empty or the content has changed, it updates the file with the new data.
    
    Args:
        tree (dict): The tree structure to save.
        
    Returns:
        None
    """
    path = get_utils_path()
    try:
        if not os.path.exists(path):
            with open(path, 'w') as file:
                json.dump({"sim_tree": tree}, file)
        else:
            with open(path, 'r') as file:
                data = json.load(file)
            
            data["sim_tree"] = tree
            with open(path, 'w') as file:
                json.dump(data, file)
    except FileNotFoundError:
        with open(path, 'w') as file:
            json.dump({"sim_tree": tree}, file)
            
def read_tree_from_json():
    """
    Reads the similarity tree from the 'utils.json' file under the 'sim_tree' key.
    
    Returns:
        dict: The similarity tree structure if found, otherwise an empty dictionary.
    """
    path = get_utils_path()
    try:
        with open(path, 'r') as file:
            data = json.load(file)
            return data.get("sim_tree", {})
    except (FileNotFoundError, json.JSONDecodeError):
        return {}
    
"""def convert_names_to_synsets(tree: Dict[str, Any]) -> Dict[wn.Synset, Any]:

    Converts the names of synsets in the tree to Synset objects.

    Args:
        tree (dict): The tree structure with synset names as keys.

    Returns:
        dict: The tree structure with Synset objects as keys.

    if not isinstance(tree, dict):
        return tree

    converted_tree = {}
    for synset_name, subtree in tree.items():
        try:
            synset = wn.synset(synset_name)
            converted_tree[synset] = convert_names_to_synsets(subtree)
        except wn.WordNetError:
            print(f"Error: Synset {synset_name} not found.")
    return converted_tree
    """
def print_tree(tree, level=0):
    for synset, subtree in tree.items():
        print('  ' * level + str(synset))
        if isinstance(subtree, dict):
            print_tree(subtree, level + 1)